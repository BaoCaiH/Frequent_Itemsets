{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#!/usr/bin/env python<br># coding: utf-8\n",
    "\n",
    "Author: Bao Cai\n",
    "\n",
    "Course: Machine Learning for Descriptive Problems\n",
    "\n",
    "Topic: Frequent Itemsets\n",
    "\n",
    "Start Date: 2020-02-27\n",
    "\n",
    "Last Save: 2020-02-28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Download the groceries.csv file from itslearning (also the file is in the folder homework inside the zip of this lecture)\n",
    "\n",
    "2. Find the frequent pair of items (2-tuples) using the naÃ¯ve, A-priori and PCY algorithms. For each of these compare the time of execution and results for supports s=10, 50, 100. Comment your results.\n",
    "\n",
    "3. For the PCY algorithm, create up to 5 compact hash tables. What is  the difference in results and time of execution for 1,2,3,4 and 5 tables? Comment your results.\n",
    "\n",
    "4. Find the final list of k-frequent items (k-tuples) for k=3,4 and 5. Experiment a bit and describe the best value for the support in each case. *Warning*: You can use any of the three algorithms, but be careful, because the algorithm can take too long if you don't chose it properly.\n",
    "\n",
    "5. Using one of the results of the previous item, for one k (k=3,4 or 5) find the possible clusters using the 1-NN criteria. Comment your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import itertools\n",
    "import numpy as np\n",
    "from time import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "path_data = 'Data/groceries.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function\n",
    "\n",
    "def read_baskets(file, k=2, verbose=True):\n",
    "    \"\"\"\n",
    "    Read a basket file, for each line is a basket.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    \n",
    "    file: str\n",
    "        Path to file.\n",
    "\n",
    "    k: int\n",
    "        Number of max items in a basket.\n",
    "    \n",
    "    verbose: boolean\n",
    "        Choose to report on progress or not.\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "\n",
    "    basket_of_k: list\n",
    "        List of baskets of size k.\n",
    "    \"\"\"\n",
    "    with open(file) as f:\n",
    "        baskets = f.readlines()\n",
    "\n",
    "    basket_of_k = []\n",
    "    n = 0\n",
    "    for basket in baskets:\n",
    "        items = basket.replace('\\n', '').split(',')\n",
    "        for itemset in itertools.combinations(items, k):\n",
    "            basket_of_k.append(frozenset(itemset))\n",
    "        if verbose:\n",
    "            n += 1\n",
    "            if n % 1000 == 0:\n",
    "                print(n, 'baskets processed')\n",
    "    return basket_of_k\n",
    "\n",
    "def naive_frequency(baskets):\n",
    "    \"\"\"\n",
    "    Return a dict of frequencies for each basket in given list.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    \n",
    "    baskets: list\n",
    "        List of baskets.\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "\n",
    "    basket_frequency: dict\n",
    "        A frequency corresponds to each basket.\n",
    "    \"\"\"\n",
    "    \n",
    "    basket_frequency = {}\n",
    "    for basket in baskets:\n",
    "        if basket not in basket_frequency:\n",
    "            basket_frequency[basket] = 0\n",
    "        basket_frequency[basket] += 1\n",
    "    return basket_frequency\n",
    "\n",
    "def frequency_threshold(basket_frequency, s=100):\n",
    "    \"\"\"\n",
    "    Return a dict of set frequencies exceed support threshold.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    \n",
    "    basket_frequency: list\n",
    "        A frequency corresponds to each basket.\n",
    "    \n",
    "    s: int\n",
    "        Support threshold\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "\n",
    "    exceed_frequency: dict\n",
    "        A dict of set frequencies exceed support threshold.\n",
    "    \"\"\"\n",
    "    \n",
    "    exceed_frequency = {}\n",
    "    k = len(list(basket_frequency.keys())[0])\n",
    "    for key, value in basket_frequency.items():\n",
    "        if value >= s:\n",
    "            exceed_frequency[key] = value\n",
    "    print('{} itemsets of size {} with frequency exceed {}'.format(\n",
    "        len(exceed_frequency), k, s\n",
    "    ))\n",
    "    return exceed_frequency\n",
    "\n",
    "def a_piori_preset(\n",
    "    k,\n",
    "    s=100,\n",
    "    larger_set=None,\n",
    "    smaller_set=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Return a union set of 2 given sets.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    \n",
    "    k: int\n",
    "        Basket's size.\n",
    "    \n",
    "    s: int\n",
    "        Support threshold.\n",
    "    \n",
    "    larger_set: dict\n",
    "        If there is a set that is larger than\n",
    "        the other one, it has to be given to this\n",
    "        variable.\n",
    "    \n",
    "    smaller_set: dict\n",
    "        A dict of itemsets and their frequencies.\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "\n",
    "    frequent_preset: set\n",
    "        A set of possible frequent itemsets.\n",
    "    \"\"\"\n",
    "    \n",
    "    basket_size = len(list(larger_set.keys())[0]) +\\\n",
    "                  len(list(smaller_set.keys())[0])\n",
    "    if basket_size != k:\n",
    "        print(\n",
    "            'The given sets cannot be combined',\n",
    "            'to produce a set of', k\n",
    "        )\n",
    "        return None\n",
    "    frequent_preset = set([\n",
    "        a.union(b)\n",
    "        for a in larger_set.keys()\n",
    "        for b in smaller_set.keys()\n",
    "    ])\n",
    "    return frequent_preset\n",
    "\n",
    "def a_piori_filter(\n",
    "    file,\n",
    "    k,\n",
    "    s=100,\n",
    "    frequent_preset=None,\n",
    "    read_baskets=read_baskets,\n",
    "    naive_frequency=naive_frequency,\n",
    "    frequency_threshold=frequency_threshold\n",
    "):\n",
    "    \"\"\"\n",
    "    Return a dict of set frequencies exceed support threshold.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    \n",
    "    file: str\n",
    "        Path to the data file.\n",
    "    \n",
    "    k: int\n",
    "        Basket's size.\n",
    "    \n",
    "    s: int\n",
    "        Support threshold.\n",
    "        \n",
    "    frequent_preset: set\n",
    "        A set of possible frequent itemsets.\n",
    "    \n",
    "    read_baskets: function\n",
    "    \n",
    "    naive_frequency: function\n",
    "    \n",
    "    frequency_threshold: function\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "\n",
    "    frequency_threshold(filtered_set, s): dict\n",
    "        A dict of set frequencies exceed support threshold.\n",
    "    \"\"\"\n",
    "    \n",
    "    if k == 1:\n",
    "        return frequency_threshold(\n",
    "            naive_frequency(\n",
    "                read_baskets(file, k, False)\n",
    "            ),\n",
    "            s\n",
    "        )\n",
    "    filtered_set = {}\n",
    "    for basket in read_baskets(file, k, False):\n",
    "        if basket not in frequent_preset:\n",
    "            continue\n",
    "        if basket not in filtered_set:\n",
    "            filtered_set[basket] = 0\n",
    "        filtered_set[basket] += 1\n",
    "        \n",
    "    return frequency_threshold(filtered_set, s)\n",
    "\n",
    "def PCY_hash(\n",
    "    file,\n",
    "    k,\n",
    "    n_hash=2,\n",
    "    s=100,\n",
    "    read_baskets=read_baskets\n",
    "):\n",
    "    \"\"\"\n",
    "    Return list of hashing tables.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    \n",
    "    file: str\n",
    "        Path to the data file.\n",
    "    \n",
    "    k: int\n",
    "        Basket's size.\n",
    "        \n",
    "    n_hash: int\n",
    "        Number of hash table.\n",
    "    \n",
    "    s: int\n",
    "        Support threshold.\n",
    "    \n",
    "    read_baskets: function\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "\n",
    "    hash_tables: list\n",
    "        A list of hashing tables.\n",
    "    \"\"\"\n",
    "    \n",
    "    hash_tables = []\n",
    "    for i in range(n_hash):\n",
    "        max_hash = 5*1000000 + i*1024\n",
    "        hash_tables.append([np.zeros((max_hash,), dtype=int), max_hash])\n",
    "    \n",
    "    for key in read_baskets(file, k, False):\n",
    "        for hash_table, max_hash in hash_tables:\n",
    "            hash_table[hash(key)%max_hash] += 1\n",
    "    for i in range(len(hash_tables)):\n",
    "        hash_tables[i][0] = set(np.where(hash_tables[i][0] >= s)[0])\n",
    "    return hash_tables\n",
    "\n",
    "def PCY_filter(\n",
    "    file,\n",
    "    k,\n",
    "    s=100,\n",
    "    hash_tables=None,\n",
    "    frequent_preset=None,\n",
    "    read_baskets=read_baskets,\n",
    "    frequency_threshold=frequency_threshold\n",
    "):\n",
    "    \"\"\"\n",
    "    Return a dict of set frequencies exceed support threshold.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    \n",
    "    file: str\n",
    "        Path to the data file.\n",
    "    \n",
    "    k: int\n",
    "        Basket's size.\n",
    "    \n",
    "    s: int\n",
    "        Support threshold.\n",
    "    \n",
    "    hash_tables: list\n",
    "        A list of hashing tables.\n",
    "        \n",
    "    frequent_preset: set\n",
    "        A set of possible frequent itemsets.\n",
    "    \n",
    "    read_baskets: function\n",
    "    \n",
    "    frequency_threshold: function\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "\n",
    "    frequency_threshold(filtered_set, s): dict\n",
    "        A dict of set frequencies exceed support threshold.\n",
    "    \"\"\"\n",
    "\n",
    "    filtered_set = {}\n",
    "    for basket in read_baskets(file, k, False):\n",
    "        if basket not in frequent_preset:\n",
    "            continue\n",
    "        accept = True\n",
    "        for hash_table, max_hash in hash_tables:\n",
    "            hashed = hash(basket)%max_hash\n",
    "            if hashed not in hash_table:\n",
    "                accept = False\n",
    "                break\n",
    "        if not accept:\n",
    "            continue\n",
    "        if basket not in filtered_set:\n",
    "            filtered_set[basket] = 0\n",
    "        filtered_set[basket] += 1\n",
    "    return frequency_threshold(filtered_set, s)\n",
    "\n",
    "class Node:\n",
    "    \"\"\"Node class for the linked list.\"\"\"\n",
    "\n",
    "    def __init__(self, value, nxt=None):\n",
    "        \"\"\"Initialize the node.\"\"\"\n",
    "        self.value = value\n",
    "        self.next = nxt\n",
    "\n",
    "def clustering(lst):\n",
    "    \"\"\"\n",
    "    Return a list of clusters.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    \n",
    "    lst: list\n",
    "        A list of pairs.\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "\n",
    "    lst_cluster: list\n",
    "        A list of clusters.\n",
    "    \"\"\"\n",
    "    \n",
    "    lst_element = []\n",
    "    lst_head = []\n",
    "    lst_node = []\n",
    "    \n",
    "    i, j = lst[0]\n",
    "    lst_element = lst_element + [i, j]\n",
    "    lst_node.append(Node(i))\n",
    "    lst_head.append(lst_node[-1])\n",
    "    lst_node.append(Node(j))\n",
    "    lst_node[-2].next = lst_node[-1]\n",
    "    n = 1\n",
    "    \n",
    "    t1 = time()\n",
    "    print('Start checking pairs')\n",
    "    c = 0\n",
    "    for i, j in lst[1:]:\n",
    "        if i in lst_element:\n",
    "            lst_element.append(j)\n",
    "            lst_node.append(Node(j))\n",
    "            n += 1\n",
    "            k = lst_element.index(i)\n",
    "            lst_node[k].next, lst_node[n].next = lst_node[n], lst_node[k].next\n",
    "        elif j in lst_element:\n",
    "            lst_element.append(i)\n",
    "            lst_node.append(Node(i))\n",
    "            n += 1\n",
    "            k = lst_element.index(j)\n",
    "            lst_node[k].next, lst_node[n].next = lst_node[n], lst_node[k].next\n",
    "        else:\n",
    "            lst_element.append(i)\n",
    "            lst_element.append(j)\n",
    "            lst_node.append(Node(i))\n",
    "            lst_head.append(lst_node[-1])\n",
    "            lst_node.append(Node(j))\n",
    "            lst_node[-2].next = lst_node[-1]\n",
    "            n += 2\n",
    "        c += 1\n",
    "        if c%100000 == 0:\n",
    "            print('100000 pairs checked in {} seconds'.format(time() - t1))\n",
    "    \n",
    "    lst_cluster = []\n",
    "    for head in lst_head:\n",
    "        temp = []\n",
    "        while head:\n",
    "            temp.append(head.value)\n",
    "            head = head.next\n",
    "        lst_cluster.append(temp)\n",
    "    return lst_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/groceries.csv') as f:\n",
    "    baskets = f.readlines()\n",
    "\n",
    "baskets = [\n",
    "    frozenset(\n",
    "        basket.replace('\\n', '').split(',')\n",
    "    ) for basket in baskets\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 baskets processed\n",
      "2000 baskets processed\n",
      "3000 baskets processed\n",
      "4000 baskets processed\n",
      "5000 baskets processed\n",
      "6000 baskets processed\n",
      "7000 baskets processed\n",
      "8000 baskets processed\n",
      "9000 baskets processed\n"
     ]
    }
   ],
   "source": [
    "baskets = read_baskets(path_data, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207 itemsets of size 2 with frequency exceed 100\n",
      "CPU times: user 83.7 ms, sys: 3.45 ms, total: 87.2 ms\n",
      "Wall time: 83.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "naive_itemsets = naive_frequency(baskets)\n",
    "naive_100_threshold = frequency_threshold(naive_itemsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "605 itemsets of size 2 with frequency exceed 50\n",
      "CPU times: user 85.5 ms, sys: 3.07 ms, total: 88.6 ms\n",
      "Wall time: 85 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "naive_itemsets = naive_frequency(baskets)\n",
    "naive_50_threshold = frequency_threshold(naive_itemsets, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1674 itemsets of size 2 with frequency exceed 20\n",
      "CPU times: user 75.2 ms, sys: 7.01 ms, total: 82.2 ms\n",
      "Wall time: 78.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "naive_itemsets = naive_frequency(baskets)\n",
    "naive_20_threshold = frequency_threshold(naive_itemsets, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2981 itemsets of size 2 with frequency exceed 10\n",
      "CPU times: user 74.1 ms, sys: 7.99 ms, total: 82.1 ms\n",
      "Wall time: 78.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "naive_itemsets = naive_frequency(baskets)\n",
    "naive_10_threshold = frequency_threshold(naive_itemsets, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31 itemsets of size 3 with frequency exceed 100\n",
      "CPU times: user 1.02 s, sys: 138 ms, total: 1.16 s\n",
      "Wall time: 1.15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "naive_k3_itemsets = naive_frequency(read_baskets(path_data, 3, False))\n",
    "naive_k3_100_threshold = frequency_threshold(naive_k3_itemsets, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1991 itemsets of size 3 with frequency exceed 20\n",
      "CPU times: user 1.19 s, sys: 44.6 ms, total: 1.23 s\n",
      "Wall time: 1.23 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "naive_k3_itemsets = naive_frequency(read_baskets(path_data, 3, False))\n",
    "naive_k3_20_threshold = frequency_threshold(naive_k3_itemsets, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "395 itemsets of size 4 with frequency exceed 20\n",
      "CPU times: user 3.58 s, sys: 186 ms, total: 3.77 s\n",
      "Wall time: 3.76 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "naive_k4_itemsets = naive_frequency(read_baskets(path_data, 4, False))\n",
    "naive_k4_20_threshold = frequency_threshold(naive_k4_itemsets, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 itemsets of size 5 with frequency exceed 20\n",
      "CPU times: user 10.1 s, sys: 1.32 s, total: 11.5 s\n",
      "Wall time: 11.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "naive_k5_itemsets = naive_frequency(read_baskets(path_data, 5, False))\n",
    "naive_k5_20_threshold = frequency_threshold(naive_k5_itemsets, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A-Piori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88 itemsets of size 1 with frequency exceed 100\n",
      "120 itemsets of size 1 with frequency exceed 50\n",
      "157 itemsets of size 1 with frequency exceed 10\n"
     ]
    }
   ],
   "source": [
    "apiori_k1_s100 = a_piori_filter(path_data, 1)\n",
    "apiori_k1_s50 = a_piori_filter(path_data, 1, 50)\n",
    "apiori_k1_s10 = a_piori_filter(path_data, 1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "apiori_k2_s100 = a_piori_preset(\n",
    "    2,\n",
    "    100,\n",
    "    apiori_k1_s100,\n",
    "    apiori_k1_s100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207 itemsets of size 2 with frequency exceed 100\n",
      "CPU times: user 273 ms, sys: 5.93 ms, total: 279 ms\n",
      "Wall time: 275 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "apiori_k2_s100 = a_piori_filter(\n",
    "    path_data,\n",
    "    2,\n",
    "    100,\n",
    "    apiori_k2_s100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "apiori_k2_s50 = a_piori_preset(\n",
    "    2,\n",
    "    50,\n",
    "    apiori_k1_s50,\n",
    "    apiori_k1_s50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "605 itemsets of size 2 with frequency exceed 50\n",
      "CPU times: user 268 ms, sys: 2.92 ms, total: 271 ms\n",
      "Wall time: 268 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "apiori_k2_s50 = a_piori_filter(\n",
    "    path_data,\n",
    "    2,\n",
    "    50,\n",
    "    apiori_k2_s50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "apiori_k2_s10 = a_piori_preset(\n",
    "    2,\n",
    "    10,\n",
    "    apiori_k1_s10,\n",
    "    apiori_k1_s10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2981 itemsets of size 2 with frequency exceed 10\n",
      "CPU times: user 288 ms, sys: 3 ms, total: 291 ms\n",
      "Wall time: 288 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "apiori_k2_s10 = a_piori_filter(\n",
    "    path_data,\n",
    "    2,\n",
    "    10,\n",
    "    apiori_k2_s10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "apiori_k3_s100 = a_piori_preset(\n",
    "    3,\n",
    "    100,\n",
    "    apiori_k2_s100,\n",
    "    apiori_k1_s100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31 itemsets of size 3 with frequency exceed 100\n",
      "CPU times: user 579 ms, sys: 21.1 ms, total: 600 ms\n",
      "Wall time: 596 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "apiori_k3_s100 = a_piori_filter(\n",
    "    path_data,\n",
    "    3,\n",
    "    100,\n",
    "    apiori_k3_s100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147 itemsets of size 1 with frequency exceed 20\n",
      "1674 itemsets of size 2 with frequency exceed 20\n"
     ]
    }
   ],
   "source": [
    "apiori_k1_s20 = a_piori_filter(path_data, 1, 20)\n",
    "apiori_k2_s20 = a_piori_preset(\n",
    "    2,\n",
    "    20,\n",
    "    apiori_k1_s20,\n",
    "    apiori_k1_s20\n",
    ")\n",
    "apiori_k2_s20 = a_piori_filter(\n",
    "    path_data,\n",
    "    2,\n",
    "    20,\n",
    "    apiori_k2_s20\n",
    ")\n",
    "\n",
    "apiori_k3_s20 = a_piori_preset(\n",
    "    3,\n",
    "    20,\n",
    "    apiori_k2_s20,\n",
    "    apiori_k1_s20\n",
    ")\n",
    "\n",
    "apiori_k4_s20 = a_piori_preset(\n",
    "    4,\n",
    "    20,\n",
    "    apiori_k2_s20,\n",
    "    apiori_k2_s20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1991 itemsets of size 3 with frequency exceed 20\n",
      "CPU times: user 10.2 s, sys: 19.1 s, total: 29.3 s\n",
      "Wall time: 1min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "apiori_k3_s20 = a_piori_filter(\n",
    "    path_data,\n",
    "    3,\n",
    "    20,\n",
    "    apiori_k3_s20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "apiori_k5_s20 = a_piori_preset(\n",
    "    5,\n",
    "    20,\n",
    "    apiori_k3_s20,\n",
    "    apiori_k2_s20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "395 itemsets of size 4 with frequency exceed 20\n",
      "CPU times: user 3.66 s, sys: 0 ns, total: 3.66 s\n",
      "Wall time: 3.79 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "apiori_k4_s20 = a_piori_filter(\n",
    "    path_data,\n",
    "    4,\n",
    "    20,\n",
    "    apiori_k4_s20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 itemsets of size 5 with frequency exceed 20\n",
      "CPU times: user 1min 25s, sys: 5min 32s, total: 6min 58s\n",
      "Wall time: 18min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "apiori_k5_s20 = a_piori_filter(\n",
    "    path_data,\n",
    "    5,\n",
    "    20,\n",
    "    apiori_k5_s20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basket size 2 and 3 seems large so I checked with Naive method above and it's checked out.\n",
    "\n",
    "Also, I think the more advance algorithm will benefit when the number of items in a single basket is relatively large or equal to the number of unique items. The reason behind that statement is that for this dataset, there's not much 5 items basket (I guess, I didn't open many of them), so naive method can just run through them and count pretty easily. I did also suspect my function made it loop too many times on the read length and all so I splited them into 2 part, the filtering part alone took quite long as well. I ran out of ideas.\n",
    "\n",
    "Then the support makes the time goes up the lower it gets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207 itemsets of size 2 with frequency exceed 100\n",
      "CPU times: user 1 s, sys: 0 ns, total: 1 s\n",
      "Wall time: 998 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "PCY_k2_s100 = a_piori_preset(\n",
    "    2,\n",
    "    100,\n",
    "    apiori_k1_s100,\n",
    "    apiori_k1_s100\n",
    ")\n",
    "PCY_k2_100 = PCY_filter(\n",
    "    path_data,\n",
    "    2,\n",
    "    s=100,\n",
    "    hash_tables=PCY_hash(\n",
    "        path_data,\n",
    "        2,\n",
    "        s=100\n",
    "    ),\n",
    "    frequent_preset=a_piori_preset(\n",
    "        2,\n",
    "        100,\n",
    "        apiori_k1_s100,\n",
    "        apiori_k1_s100\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "605 itemsets of size 2 with frequency exceed 50\n",
      "CPU times: user 1.11 s, sys: 0 ns, total: 1.11 s\n",
      "Wall time: 1.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "PCY_k2_s50 = a_piori_preset(\n",
    "    2,\n",
    "    50,\n",
    "    apiori_k1_s50,\n",
    "    apiori_k1_s50\n",
    ")\n",
    "PCY_k2_50 = PCY_filter(\n",
    "    path_data,\n",
    "    2,\n",
    "    s=50,\n",
    "    hash_tables=PCY_hash(\n",
    "        path_data,\n",
    "        2,\n",
    "        s=50\n",
    "    ),\n",
    "    frequent_preset=a_piori_preset(\n",
    "        2,\n",
    "        50,\n",
    "        apiori_k1_s50,\n",
    "        apiori_k1_s50\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2981 itemsets of size 2 with frequency exceed 10\n",
      "CPU times: user 3.9 s, sys: 0 ns, total: 3.9 s\n",
      "Wall time: 3.87 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "PCY_k2_s10 = a_piori_preset(\n",
    "    2,\n",
    "    10,\n",
    "    apiori_k1_s10,\n",
    "    apiori_k1_s10\n",
    ")\n",
    "PCY_k2_10 = PCY_filter(\n",
    "    path_data,\n",
    "    2,\n",
    "    s=10,\n",
    "    hash_tables=PCY_hash(\n",
    "        path_data,\n",
    "        2,\n",
    "        s=10\n",
    "    ),\n",
    "    frequent_preset=a_piori_preset(\n",
    "        2,\n",
    "        10,\n",
    "        apiori_k1_s10,\n",
    "        apiori_k1_s10\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207 itemsets of size 2 with frequency exceed 100\n",
      "CPU times: user 1.2 s, sys: 0 ns, total: 1.2 s\n",
      "Wall time: 1.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "PCY_k2_s100 = a_piori_preset(\n",
    "    2,\n",
    "    100,\n",
    "    apiori_k1_s100,\n",
    "    apiori_k1_s100\n",
    ")\n",
    "PCY_k2_100 = PCY_filter(\n",
    "    path_data,\n",
    "    2,\n",
    "    s=100,\n",
    "    hash_tables=PCY_hash(\n",
    "        path_data,\n",
    "        2,\n",
    "        n_hash=3,\n",
    "        s=100\n",
    "    ),\n",
    "    frequent_preset=a_piori_preset(\n",
    "        2,\n",
    "        100,\n",
    "        apiori_k1_s100,\n",
    "        apiori_k1_s100\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207 itemsets of size 2 with frequency exceed 100\n",
      "CPU times: user 1.3 s, sys: 0 ns, total: 1.3 s\n",
      "Wall time: 1.29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "PCY_k2_s100 = a_piori_preset(\n",
    "    2,\n",
    "    100,\n",
    "    apiori_k1_s100,\n",
    "    apiori_k1_s100\n",
    ")\n",
    "PCY_k2_100 = PCY_filter(\n",
    "    path_data,\n",
    "    2,\n",
    "    s=100,\n",
    "    hash_tables=PCY_hash(\n",
    "        path_data,\n",
    "        2,\n",
    "        n_hash=4,\n",
    "        s=100\n",
    "    ),\n",
    "    frequent_preset=a_piori_preset(\n",
    "        2,\n",
    "        100,\n",
    "        apiori_k1_s100,\n",
    "        apiori_k1_s100\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207 itemsets of size 2 with frequency exceed 100\n",
      "CPU times: user 1.41 s, sys: 0 ns, total: 1.41 s\n",
      "Wall time: 1.41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "PCY_k2_s100 = a_piori_preset(\n",
    "    2,\n",
    "    100,\n",
    "    apiori_k1_s100,\n",
    "    apiori_k1_s100\n",
    ")\n",
    "PCY_k2_100 = PCY_filter(\n",
    "    path_data,\n",
    "    2,\n",
    "    s=100,\n",
    "    hash_tables=PCY_hash(\n",
    "        path_data,\n",
    "        2,\n",
    "        n_hash=5,\n",
    "        s=100\n",
    "    ),\n",
    "    frequent_preset=a_piori_preset(\n",
    "        2,\n",
    "        100,\n",
    "        apiori_k1_s100,\n",
    "        apiori_k1_s100\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More hash tables means more comparison so in the process itself, the more hash table the longer it takes. Still, I think the reason why it took too long is still because of the small basket in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1674 frequent sets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[frozenset({'citrus fruit', 'semi-finished bread'}),\n",
       " frozenset({'citrus fruit', 'margarine'}),\n",
       " frozenset({'margarine', 'semi-finished bread'}),\n",
       " frozenset({'tropical fruit', 'yogurt'}),\n",
       " frozenset({'coffee', 'tropical fruit'}),\n",
       " frozenset({'coffee', 'yogurt'}),\n",
       " frozenset({'pip fruit', 'yogurt'}),\n",
       " frozenset({'cream cheese', 'pip fruit'}),\n",
       " frozenset({'cream cheese', 'yogurt'}),\n",
       " frozenset({'other vegetables', 'whole milk'}),\n",
       " frozenset({'condensed milk', 'other vegetables'}),\n",
       " frozenset({'long life bakery product', 'other vegetables'}),\n",
       " frozenset({'condensed milk', 'whole milk'}),\n",
       " frozenset({'long life bakery product', 'whole milk'}),\n",
       " frozenset({'butter', 'whole milk'}),\n",
       " frozenset({'whole milk', 'yogurt'}),\n",
       " frozenset({'rice', 'whole milk'}),\n",
       " frozenset({'butter', 'yogurt'}),\n",
       " frozenset({'rice', 'yogurt'}),\n",
       " frozenset({'UHT-milk', 'other vegetables'})]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I'll do clustering on this\n",
    "print(len(apiori_k2_s20), 'frequent sets')\n",
    "list(apiori_k2_s20.keys())[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start checking pairs\n",
      "['semi-finished bread', 'pip fruit', 'soda', 'newspapers', 'root vegetables', 'sausage', 'whipped/sour cream', 'shopping bags', 'fruit/vegetable juice', 'curd', 'citrus fruit', 'potted plants', 'bottled beer', 'grapes', 'herbs', 'pickled vegetables', 'shopping bags', 'salty snack', 'baking powder', 'soda']\n",
      "\n",
      "['tropical fruit', 'specialty bar', 'chewing gum', 'roll products', 'mustard', 'cat food', 'canned beer', 'dessert', 'ice cream', 'specialty chocolate', 'potted plants', 'long life bakery product', 'canned fish', 'grapes', 'seasonal products', 'UHT-milk', 'frozen vegetables', 'beverages', 'red/blush wine', 'beef']\n",
      "\n",
      "['whole milk', 'house keeping products', 'liver loaf', 'kitchen towels', 'popcorn', 'instant coffee', 'jam', 'female sanitary products', 'chewing gum', 'turkey', 'sauces', 'white wine', 'chocolate marshmallow', 'ice cream', 'Instant food products', 'sweet spreads', 'curd cheese', 'mayonnaise', 'cleaner', 'specialty cheese']\n",
      "\n",
      "['pastry', 'canned fish', 'ice cream', 'oil', 'flour', 'potted plants', 'meat', 'salty snack', 'sliced cheese', 'processed cheese', 'red/blush wine', 'hygiene articles', 'bottled beer', 'napkins', 'whipped/sour cream', 'pip fruit', 'fruit/vegetable juice', 'semi-finished bread', 'berries', 'newspapers']\n",
      "\n",
      "['pork', 'sliced cheese', 'candy', 'salty snack', 'flour', 'bottled beer', 'meat', 'domestic eggs', 'butter milk', 'newspapers', 'napkins', 'fruit/vegetable juice', 'hygiene articles', 'curd', 'oil', 'shopping bags', 'root vegetables', 'herbs', 'pip fruit', 'pastry']\n",
      "\n",
      "['beef', 'waffles', 'cream cheese', 'frozen meals', 'chicken', 'salty snack', 'fruit/vegetable juice', 'chocolate', 'sugar', 'shopping bags', 'soda', 'onions', 'newspapers', 'coffee', 'oil', 'domestic eggs', 'pastry', 'white bread', 'napkins', 'hygiene articles']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for cluster in clustering(list(apiori_k2_s20.keys())):\n",
    "    print(cluster[:20], end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there are about 6 clusters:\n",
    "\n",
    "- The first cluster looks like a healthy life style\n",
    "- The second one, not so much, a lot of sugar in take\n",
    "- The third one belongs to the ladies\n",
    "\n",
    "The rest are quite random to me. Also there are some overlapping items that appeared in more than 1 cluster like `meat` which also make sense."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
